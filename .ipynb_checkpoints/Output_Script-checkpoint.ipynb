{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to Produce an Output from the Models\n",
    "Presumably this will need to be transferred to a script-based format but this is the code to turn an input image into a controller action\n",
    "\n",
    "## Loading Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Flatten, Input, Dense, Dropout, Lambda, Reshape, MaxPooling2D, LSTM, Reshape\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables for potential modification\n",
    "\n",
    "`img_height/width`: this and all below variables should be the same for the trained images and input images\n",
    "\n",
    "`z_len`: the length of the image compression made by the encoder\n",
    "`a_len`: the length of the action vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 128\n",
    "img_width = 128\n",
    "num_channels = 1\n",
    "input_shape = (img_height, img_width, num_channels)\n",
    "z_len = 2048\n",
    "a_len = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures\n",
    "\n",
    "# VAE\n",
    "\n",
    "load the vae (have to make the architecture again, make sure the code below matches the code in the Data Prepper/VAE Trainer)\n",
    "\n",
    "\n",
    "# Encoder\n",
    "`latent_dim`: changing this will make the model exponentially larger or smaller\n",
    "\n",
    "`x`: the model (saved in x)\n",
    "\n",
    "`conv_shape`: Shape of conv to be provided to decoder\n",
    "\n",
    "`z_mu` and `z_sigma`: Two outputs, for latent mean and log variance (std. dev.) Use these to sample random variables in latent space to which inputs are mapped. \n",
    "\n",
    "`sample_z` Function: REPARAMETERIZATION TRICK\n",
    "- Define sampling function to sample from the distribution\n",
    "- Reparameterize sample based on the process defined by Gunderson and Huang\n",
    "- into the shape of: mu + sigma squared x eps\n",
    "- This is to allow gradient descent to allow for gradient estimation accurately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2048\n",
    "\n",
    "input_img = Input(shape=input_shape, name='encoder_input')\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(input_img)\n",
    "x = MaxPooling2D((2,2), padding='same')\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "conv_shape = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(latent_dim*2, activation='relu')(x)\n",
    "\n",
    "z_mu = Dense(latent_dim, name='latent_mu')(x)\n",
    "z_sigma = Dense(latent_dim, name='latent_sigma')(x)\n",
    "\n",
    "def sample_z(args):\n",
    "    z_mu, z_sigma = args\n",
    "    eps = K.random_normal(shape=(K.shape(z_mu)[0], K.int_shape(z_mu)[1]))\n",
    "    return z_mu + K.exp(z_sigma / 2) * eps\n",
    "\n",
    "z = Lambda(sample_z, output_shape=(latent_dim,), name='z')([z_mu, z_sigma])\n",
    "\n",
    "encoder = Model(input_img, [z_mu, z_sigma, z], name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder \n",
    "\n",
    "### decoder takes the latent vector as input: `decoder_input`\n",
    "\n",
    "First `x` Layer: Need to start with a shape that can be remapped to original image shape as we want our final utput to be same shape original input. So, add dense layer with dimensions that can be reshaped to desired output shape\n",
    "\n",
    "Second `x` Layer: reshape to the shape of last conv. layer in the encoder, so we can \n",
    "\n",
    "Third `x` Layer: upscale (conv2D transpose) back to original shape use Conv2DTranspose to reverse the conv layers defined in the encoder\n",
    "\n",
    "Last `x` Layer: Using sigmoid activation (Can add more conv2DTranspose layers, if desired.)\n",
    "\n",
    "### **`decoder`**: \n",
    "**Define and Summarize DECODER Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(latent_dim, ), name='decoder_input')\n",
    "\n",
    "x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation='relu')(decoder_input)\n",
    "x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
    "x = Conv2DTranspose(32, 3, padding='same', activation='relu', strides=(2,2))(x)\n",
    "x = Conv2DTranspose(32, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu', strides=(2,2))(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu', strides=(2,2))(x)\n",
    "\n",
    "x = Conv2DTranspose(num_channels, 3, padding='same', activation='sigmoid', name='decoder_output')(x)\n",
    "\n",
    "decoder = Model(decoder_input, x, name='decoder')\n",
    "\n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Loss Function\n",
    "\n",
    "- `recon_loss`: Reconstruction loss (as we used sigmoid activation we can use binarycrossentropy)\n",
    "\n",
    "- `kl_loss`: KL Divergence\n",
    "\n",
    "- `call()` Function: ADDs a CUSTOM Loss to the Class\n",
    "\n",
    "- `y`: APPLY the Custom Loss to the Input Images and the Decoded Latent Distribution Sample: **`y` is basically the original image after encoding input img to `mu`, `sigma`, `z` and decoding sampled `z` values. This will be used as output for vae**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomLayer(keras.layers.Layer):\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        \n",
    "        recon_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        \n",
    "        kl_loss = -5e-4 * K.mean(1 + z_sigma - K.square(z_mu) - K.exp(z_sigma), axis=-1)\n",
    "        return K.mean(recon_loss + kl_loss)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae.loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "\n",
    "y = CustomLayer()([input_img, z_decoded])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Model\n",
    "\n",
    "### **`input_to_rnn`, `x`, and `rnn_output`: RNN Layers**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_rnn = Input(shape=(1,z_len))\n",
    "\n",
    "x = LSTM(z_len, return_sequences=True)(input_to_rnn)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "rnn_output = Dense(2048, activation='sigmoid')(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controller\n",
    "\n",
    "`input_to_controller`, `x`, and `ctrl_output`: Controller's LAYERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_controller = Input(shape=(1, z_len*2))\n",
    "\n",
    "x = Dense(z_len)(input_to_controller)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len/2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len/4)(input_to_controller)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len/16)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "ctrl_output = Dense(a_len, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model LOADING\n",
    "\n",
    "1. Load Encoder (`encoder`)\n",
    "2. Load RNN (`rnn`)\n",
    "3. Load Controller (`ctrl`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(input_img, y, name='vae')\n",
    "vae.load_weights(os.getcwd() + \"\\\\models\\\\vae.h5\")\n",
    "encoder = Model(vae.input, vae.layers[15].output)\n",
    "rnn = Model(input_to_rnn, rnn_output, name='rnn')\n",
    "rnn.load_weights(os.getcwd() + \"\\\\models\\\\rnn.h5\")\n",
    "ctrl = Model(input_to_controller, ctrl_output, name='controller')\n",
    "ctrl.load_weights(os.getcwd() + \"\\\\models\\\\cntrl.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Input/Output**\n",
    "\n",
    "1. `img_path`: right now, will need to have saved an image to run it through the models edit **`img_path`** for desired image to process\n",
    "2. `img_array`: loading in image and reshaping\n",
    "3. **`img_array.reshape(int, int, int)`**: the **THIRD** Argument (set to 1) - this 1 might need to be num_channels, but I'm not sure. If num_channels is ever not set to 1, keep an eye on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.getcwd() + \"\\\\images\\\\2021-02-27\\\\2021-02-27-1350-01-24-22.NEF\"\n",
    "\n",
    "img_array = cv2.imread(img_path)\n",
    "img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)\n",
    "img_array = cv2.resize(img_array, (img_height, img_width))\n",
    "img_array = img_array.reshape(-1, img_height, img_width, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "`z`, `zprime`, and `action`: predictions by each piece of the model\n",
    "\n",
    "1. `z`: encode image\n",
    "2. `z.reshape()`: reshape for the RNN\n",
    "3. `zprime`: RNN Prediction\n",
    "4. `z_and_zprime`: concat for controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = encoder.predict(img_array)\n",
    "z = z.reshape(-1, 1, 2048)\n",
    "zprime = rnn.predict(z)\n",
    "z_and_zprime = np.reshape(np.concatenate((z[0][0], zprime[0][0])), (1, z_len*2))[None,:,:]\n",
    "action = ctrl.predict(z_and_zprime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"z: \", z)\n",
    "print(\"z': \", zprime)\n",
    "print(\"action: \", action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
