{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessor for the Controller\n",
    "The controller receives the z produced by the encoder (from the VAE) and the z' produced by the RNN's prediction to determine an action.\n",
    "The below processor takes a collection of image data and the action performed for preprocessing. It finds z and z' by putting the image through the encoder and then putting z through the RNN.\n",
    "In the form I left it in, we did not have a good dataset of images and corresponding images. Right now, it just sets every action to 22 (no action).\n",
    "\n",
    "## Loading stuff into Kernal\n",
    "\n",
    "imports\n",
    "\n",
    "`import sys`: added so that cv2 gets installed in kernal\n",
    "\n",
    "`import sys`\n",
    "`!{sys.executable} -m pip install opencv-python`\n",
    "commented the above code, it started working, \n",
    "idk why if code not working try uncommenting the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape, MaxPooling2D, Dropout, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "# added so that cv2 gets installed in kernal\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install opencv-python\n",
    "# commented the above code, it started working, idk why\n",
    "# if code not working try uncommenting the above\n",
    "import cv2\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (preprocessed from Data Processing Script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = joblib.load(\"images/train_data.z\")\n",
    "print(train_data.shape[2])\n",
    "\n",
    "img_width = train_data.shape[1]\n",
    "img_height = train_data.shape[2]\n",
    "num_channels = 1\n",
    "x_train = train_data.reshape(train_data.shape[0], img_height, img_width, num_channels)\n",
    "\n",
    "input_shape = (img_height, img_width, num_channels)\n",
    "print(input_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the vae (have to make the architecture again, make sure the code below\n",
    "\n",
    "matches the code in the Data Prepper/VAE Trainer)\n",
    "\n",
    "\n",
    "# Encoder\n",
    "\n",
    "`x`: The Encoder Model\n",
    "\n",
    "`latent_dim`: changing this will make the model exponentially larger or smaller\n",
    "\n",
    "`conv_shape`: Shape of conv to be provided to decoder\n",
    "\n",
    "`z_mu` and `z_sigma`: Two outputs, for latent mean and log variance (std. dev.)\n",
    "Use these to sample random variables in latent space to which inputs\n",
    "\n",
    "`z_mu`: Mean values of encoded input\n",
    "`z_sigma`: Std dev. (variance) of encoded input\n",
    "\n",
    "`sample_z()` Function: REPARAMETERIZATION TRICK\n",
    "- Define sampling function to sample from the distribution\n",
    "- Reparameterize sample based on the process defined by Gunderson and Huang\n",
    "- into the shape of: mu + sigma squared x eps\n",
    "- This is to allow gradient descent to allow for gradient estimation accurately. \n",
    "\n",
    "`z`: sample vector from the latent distribution `z` is the labda custom layer we are adding for gradient descent calculations using mu and variance (sigma)\n",
    "\n",
    "`encoder`: Z (lambda layer) will be the last layer in the encoder. Define and summarize encoder model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2048\n",
    "\n",
    "input_img = Input(shape=input_shape, name='encoder_input')\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(input_img)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "conv_shape = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(latent_dim*2, activation='relu')(x)\n",
    "\n",
    "z_mu = Dense(latent_dim, name='latent_mu')(x)\n",
    "z_sigma = Dense(latent_dim, name='latent_sigma')(x)\n",
    "\n",
    "def sample_z(args):\n",
    "    z_mu, z_sigma = args\n",
    "    eps = K.random_normal(shape=(K.shape(z_mu)[0], K.int_shape(z_mu)[1]))\n",
    "    return z_mu + K.exp(z_sigma / 2) * eps\n",
    "\n",
    "z = Lambda(sample_z, output_shape=(latent_dim,), name='z')([z_mu, z_sigma])\n",
    "\n",
    "encoder = Model(input_img, [z_mu, z_sigma, z], name='encoder')\n",
    "print(encoder.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "decoder takes the latent vector as input\n",
    "\n",
    "`x`: Need to start with a shape that can be remapped to original image shape as we want our final utput to be same shape original input. So, add dense layer with dimensions that can be reshaped to desired output shape\n",
    "\n",
    "`x = Reshape()`: reshape to the shape of last conv. layer in the encoder, so we can upscale (conv2D transpose) back to original shape use Conv2DTranspose to reverse the conv layers defined in the encoder\n",
    "(`Conv2DTranspose()` Layers)\n",
    "\n",
    "Last `Conv2DTranspose()` Layer: Using sigmoid activation  (Can add more conv2DTranspose layers, if desired.)\n",
    "\n",
    "`z_decoded`: apply the decoder to the latent sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(latent_dim, ), name='decoder_input')\n",
    "\n",
    "x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation='relu')(decoder_input)\n",
    "x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
    "x = Conv2DTranspose(32, 3, padding='same', activation='relu', strides=(2,2))(x)\n",
    "x = Conv2DTranspose(32, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu', strides=(2,2))(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu', strides=(2,2))(x)\n",
    "\n",
    "x = Conv2DTranspose(num_channels, 3, padding='same', activation='sigmoid', name='decoder_output')(x)\n",
    "\n",
    "decoder = Model(decoder_input, x, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "- `recon_loss`: Reconstruction loss (as we used sigmoid activation we can use binarycrossentropy)\n",
    "- `kl_loss`: KL divergence\n",
    "- `call()`: add custom loss to the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomLayer(keras.layers.Layer):\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        \n",
    "        recon_loss = keras.metrics.binary_crossentropy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
