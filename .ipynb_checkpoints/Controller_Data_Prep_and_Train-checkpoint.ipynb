{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessor for the Controller\n",
    "The controller receives the z produced by the encoder (from the VAE) and the z' produced by the RNN's prediction to determine an action.\n",
    "The below processor takes a collection of image data and the action performed for preprocessing. It finds z and z' by putting the image through the encoder and then putting z through the RNN.\n",
    "In the form I left it in, we did not have a good dataset of images and corresponding images. Right now, it just sets every action to 22 (no action).\n",
    "\n",
    "## Loading stuff into Kernal\n",
    "\n",
    "imports\n",
    "\n",
    "`import sys`: added so that cv2 gets installed in kernal\n",
    "\n",
    "`import sys`\n",
    "`!{sys.executable} -m pip install opencv-python`\n",
    "commented the above code, it started working, \n",
    "idk why if code not working try uncommenting the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape, MaxPooling2D, Dropout, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "# added so that cv2 gets installed in kernal\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install opencv-python\n",
    "# commented the above code, it started working, idk why\n",
    "# if code not working try uncommenting the above\n",
    "import cv2\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (preprocessed from Data Processing Script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = joblib.load(\"images/train_data.z\")\n",
    "print(train_data.shape[2])\n",
    "\n",
    "img_width = train_data.shape[1]\n",
    "img_height = train_data.shape[2]\n",
    "num_channels = 1\n",
    "x_train = train_data.reshape(train_data.shape[0], img_height, img_width, num_channels)\n",
    "\n",
    "input_shape = (img_height, img_width, num_channels)\n",
    "print(input_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the vae (have to make the architecture again, make sure the code below\n",
    "\n",
    "matches the code in the Data Prepper/VAE Trainer)\n",
    "\n",
    "\n",
    "# Encoder\n",
    "\n",
    "`x`: The Encoder Model\n",
    "\n",
    "`latent_dim`: changing this will make the model exponentially larger or smaller\n",
    "\n",
    "`conv_shape`: Shape of conv to be provided to decoder\n",
    "\n",
    "`z_mu` and `z_sigma`: Two outputs, for latent mean and log variance (std. dev.)\n",
    "Use these to sample random variables in latent space to which inputs\n",
    "\n",
    "`z_mu`: Mean values of encoded input\n",
    "`z_sigma`: Std dev. (variance) of encoded input\n",
    "\n",
    "`sample_z()` Function: REPARAMETERIZATION TRICK\n",
    "- Define sampling function to sample from the distribution\n",
    "- Reparameterize sample based on the process defined by Gunderson and Huang\n",
    "- into the shape of: mu + sigma squared x eps\n",
    "- This is to allow gradient descent to allow for gradient estimation accurately. \n",
    "\n",
    "`z`: sample vector from the latent distribution `z` is the labda custom layer we are adding for gradient descent calculations using mu and variance (sigma)\n",
    "\n",
    "`encoder`: Z (lambda layer) will be the last layer in the encoder. Define and summarize encoder model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2048\n",
    "\n",
    "input_img = Input(shape=input_shape, name='encoder_input')\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(input_img)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "conv_shape = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(latent_dim*2, activation='relu')(x)\n",
    "\n",
    "z_mu = Dense(latent_dim, name='latent_mu')(x)\n",
    "z_sigma = Dense(latent_dim, name='latent_sigma')(x)\n",
    "\n",
    "def sample_z(args):\n",
    "    z_mu, z_sigma = args\n",
    "    eps = K.random_normal(shape=(K.shape(z_mu)[0], K.int_shape(z_mu)[1]))\n",
    "    return z_mu + K.exp(z_sigma / 2) * eps\n",
    "\n",
    "z = Lambda(sample_z, output_shape=(latent_dim,), name='z')([z_mu, z_sigma])\n",
    "\n",
    "encoder = Model(input_img, [z_mu, z_sigma, z], name='encoder')\n",
    "print(encoder.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "decoder takes the latent vector as input\n",
    "\n",
    "`x`: Need to start with a shape that can be remapped to original image shape as we want our final utput to be same shape original input. So, add dense layer with dimensions that can be reshaped to desired output shape\n",
    "\n",
    "`x = Reshape()`: reshape to the shape of last conv. layer in the encoder, so we can upscale (conv2D transpose) back to original shape use Conv2DTranspose to reverse the conv layers defined in the encoder\n",
    "(`Conv2DTranspose()` Layers)\n",
    "\n",
    "Last `Conv2DTranspose()` Layer: Using sigmoid activation  (Can add more conv2DTranspose layers, if desired.)\n",
    "\n",
    "`z_decoded`: apply the decoder to the latent sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(latent_dim, ), name='decoder_input')\n",
    "\n",
    "x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation='relu')(decoder_input)\n",
    "x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
    "x = Conv2DTranspose(32, 3, padding='same', activation='relu', strides=(2,2))(x)\n",
    "x = Conv2DTranspose(32, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu', strides=(2,2))(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu', strides=(2,2))(x)\n",
    "\n",
    "x = Conv2DTranspose(num_channels, 3, padding='same', activation='sigmoid', name='decoder_output')(x)\n",
    "\n",
    "decoder = Model(decoder_input, x, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "- `recon_loss`: Reconstruction loss (as we used sigmoid activation we can use binarycrossentropy)\n",
    "- `kl_loss`: KL divergence\n",
    "- `call()`: add custom loss to the class\n",
    "- `y`: apply the custom loss to the input images and the decoded latent distribution sample. y is basically the original image after encoding input img to mu, sigma, z and decoding sampled z values. This will be used as output for vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomLayer(keras.layers.Layer):\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        \n",
    "        recon_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        \n",
    "        kl_loss = -5e-4 * K.mean(1 + z_sigma - K.square(z_mu) - K.exp(z_sigma), axis-1)\n",
    "        return K.mean(recon_loss + kl_loss)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "    \n",
    "\n",
    "y = CustomLayer()([input_img, z_decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(input_img, y, name='vae')\n",
    "vae.load_weights(os.getcwd() + \"\\\\models\\\\vae.h5\")\n",
    "encoder = Model(vae.input, vae.layers[15].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load RNN preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load(\"images/train_data_rnn.z\")\n",
    "train_data = np.array([np.array(p[0])for p in data])\n",
    "answers = np.array([np.array(p[0]) for p in data])\n",
    "print(train_data[0])\n",
    "print(np.shape(train_data[0]))\n",
    "z_len = np.shape(train_data[0])[-1]\n",
    "print(z_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rebuilt RNN\n",
    "\n",
    "Rebuild RNN architecture (make sure this matches architecture in RNN Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_rnn = Input(shape=(1,z_len))\n",
    "x = LSTM(2048, return_sequences=True)(input_to_rnn)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(2048)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "output = Dense(2048, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Model(input_to_rnn, output, name='rnn')\n",
    "rnn.load_weights(os.getcwd() + \"\\\\models\\\\rnn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puts all images in list\n",
    "os.chdir(\"images\")\n",
    "\n",
    "# constant for sizing\n",
    "IMG_SIZE = 128\n",
    "\n",
    "if os.path.exists(\"train_data_controller.z\"):\n",
    "    print(\"train_data_controller.z already exists, if this notebook is run to completion the old data will be replaced.\")\n",
    "\n",
    "# put images here\n",
    "data = []\n",
    "\n",
    "# loop for loading images\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "count = 0\n",
    "for folder in os.listdir(path):\n",
    "    if \"train_data\" in folder:  # skips any train data files, as that should be the only non-directory item in images\n",
    "        continue\n",
    "    print(\"FOLDER: \",folder)\n",
    "    # added + \"/\" + to below to make it work\n",
    "    for filename in os.listdir(path + \"/\" + folder):\n",
    "        # changed to NEF (That's what I have the images saved as, may need to change back to JPG in future)\n",
    "        if(\".NEF\" in filename):\n",
    "            # added slash here too\n",
    "            temp_path = path + \"/\" + folder + \"/\" + filename\n",
    "            try:\n",
    "                img_array = cv2.imread(temp_path)\n",
    "                img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)\n",
    "                img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                data.append(img_array)\n",
    "                count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "print(\"images processed:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some reshaping so the images can be encoded\n",
    "\n",
    "train_data = data\n",
    "train_data = np.array(train_data).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "train_data = train_data/255.0\n",
    "print(train_data.shape[2])\n",
    "\n",
    "# Reshape \n",
    "img_width  = train_data.shape[1]\n",
    "img_height = train_data.shape[2]\n",
    "num_channels = 1\n",
    "x_train = train_data.reshape(train_data.shape[0], img_height, img_width, num_channels)\n",
    "\n",
    "input_shape = (img_height, img_width, num_channels)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the z vectors by running training_data (the images) through the encoder\n",
    "# note: definiately not the most efficient way to do this\n",
    "z_vals = []\n",
    "for img in train_data:\n",
    "    z_vals.append(encoder.predict(img[None,:,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the z' vectors by running z_vals (the latent vectors) through the RNN\n",
    "# again, efficiency could be better\n",
    "zprime_vals = []\n",
    "for z in z_vals:\n",
    "    zprime_vals.append(rnn.predict(z[None,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the action taken\n",
    "# this code currently just labels the images as 22 so I can make the skeleton of the controller\n",
    "#    Will need to update this to properly consider actions in the future, see readme for notes\n",
    "actions = []\n",
    "for z, zprime in zip(z_vals, zprime_vals):\n",
    "    actions.append([22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes list of lists for use in training, where each entry is a z, z', action tuple\n",
    "final_data = []\n",
    "for z, zprime, action in zip(z_vals, zprime_vals, actions):\n",
    "    final_data.append([z, zprime, action])\n",
    "print(final_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished, now zip the three lists and save the data in the images directory\n",
    "print(os.getcwd())\n",
    "joblib.dump(final_data, \"train_data_controller.z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controller  \n",
    "\n",
    "## TRAINER\n",
    "\n",
    "(This part of the model does not involve any visual processing)\n",
    "\n",
    "## Loading stuff into the kernel\n",
    "Import and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape, MaxPooling2D,LSTM, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import save_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "import cv2\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "data = joblib.load(\"images/train_data_controller.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting data\n",
    "# note: right now just flattens z and z' into 1 vector, the commented out code is to keep the two vectors seperate\n",
    "#   I couldn't figure out how to reduce a 2x2048 input into a 1d one in a neural net\n",
    "# train_data = np.array([np.array([p[0][0], p[1][0][0]]) for p in data])\n",
    "train_data = np.array([[np.concatenate((p[0][0], p[1][0][0]))] for p in data])\n",
    "answers = np.array([p[2] for p in data])\n",
    "print(train_data[0], answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controller Build\n",
    "\n",
    "## Layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_len = int(len(train_data[0][0])/2)\n",
    "a_len = len(answers[0])\n",
    "\n",
    "input_to_controller = Input(shape=(1, z_len*2))\n",
    "\n",
    "x = Dense(z_len)(input_to_controller)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len/2)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len/4)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len/16)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "output = Dense(a_len, activation='sigmoid')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntrl = Model(input_to_controller, output, name='controller')\n",
    "cntrl.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntrl.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntrl.fit(train_data, answers, epochs=10, verbose=1, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntrl.save_weights('models/cntrl.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
