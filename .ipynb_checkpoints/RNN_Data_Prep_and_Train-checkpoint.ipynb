{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessor for the RNN\n",
    "The RNN takes z, the encoding of a certain image, and predicts z', the encoding of the image following. So, to make data to train the RNN on, we need to make pairs of z and z' from our pictoral data.\n",
    "\n",
    "## Loading stuff into the Kernel\n",
    "\n",
    "Import Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "# added so that cv2 gets installed in kernal\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install opencv-python\n",
    "# commented the above code, it started working, idk why\n",
    "# if code not working try uncommenting the above\n",
    "import cv2\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data (preprocessed from Data Processing Script)\n",
    "\n",
    "## (And Reshape `x_train`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_data = joblib.load(\"images/train_data.z\")\n",
    "print(train_data.shape[2])\n",
    "\n",
    "\n",
    "img_width  = train_data.shape[1]\n",
    "img_height = train_data.shape[2]\n",
    "num_channels = 1\n",
    "x_train = train_data.reshape(train_data.shape[0], img_height, img_width, num_channels)\n",
    "\n",
    "input_shape = (img_height, img_width, num_channels)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder  \n",
    "\n",
    "load the vae (have to make the architecture again, make sure the code below matches the code in the Data Prepper/VAE Trainer)\n",
    "\n",
    "## `sample_z()` Function:\n",
    "\n",
    "### REPARAMETERIZATION TRICK\n",
    "\n",
    "Define sampling function to sample from the distribution  Reparameterize sample based on the process defined by Gunderson and Huang into the shape of: mu + sigma squared x eps \n",
    "\n",
    "This is to allow gradient descent to allow for gradient estimation accurately.\n",
    "\n",
    "### `z`:  \n",
    "sample vector from the latent distribution \n",
    "z is the labda custom layer we are adding for gradient descent calculations using mu and variance (sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2048\n",
    "\n",
    "input_img = Input(shape=input_shape, name='encoder_input')\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(input_img)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "x = MaxPooling2D((2,2), padding='same')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "\n",
    "conv_shape = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(latent_dim*2, activation='relu')(x)\n",
    "\n",
    "z_mu = Dense(latent_dim, name='latent_mu')(x)\n",
    "z_sigma = Dense(latent_dim, name='latent_sigma')(x)\n",
    "\n",
    "def sample_z(args):\n",
    "    z_mu, z_sigma = args\n",
    "    eps = K.random_normal(shape=(K.shape(z_mu)[0], K.int_shape(z_mu)[1]))\n",
    "    return z_mu + K.exp(z_sigma / 2) * eps\n",
    "\n",
    "z = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([z_mu, z_sigma])\n",
    "\n",
    "encoder = Model(input_img, [z_mu, z_sigma, z], name='encoder')\n",
    "print(encoder.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder \n",
    "\n",
    "`decoder_input` : decoder takes the latent vector as input\n",
    "\n",
    "`x = Dense()`: Need to start with a shape that can be remapped to original image shape as we want our final utput to be same shape original input. So, add dense layer with dimensions that can be reshaped to desired output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation='relu')(decoder_input)\n",
    "# reshape to the shape of last conv. layer in the encoder, so we can \n",
    "x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
    "# upscale (conv2D transpose) back to original shape\n",
    "# use Conv2DTranspose to reverse the conv layers defined in the encoder\n",
    "x = Conv2DTranspose(32, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
    "x = Conv2DTranspose(32, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu')(x)\n",
    "x = Conv2DTranspose(64, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
    "#Can add more conv2DTranspose layers, if desired. \n",
    "#Using sigmoid activation\n",
    "x = Conv2DTranspose(num_channels, 3, padding='same', activation='sigmoid', name='decoder_output')(x)\n",
    "\n",
    "# Define and summarize decoder model\n",
    "decoder = Model(decoder_input, x, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# apply the decoder to the latent sample \n",
    "z_decoded = decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "## `y`: \n",
    "- apply the custom loss to the input images and the decoded latent distribution sample\n",
    "- y is basically the original image after encoding input img to mu, sigma, z and decoding sampled z values. This will be used as output for vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(keras.layers.Layer):\n",
    "    def vae_loss(self, x, z_decoded):\n",
    "        x = K.flatten(x)\n",
    "        z_decoded = K.flatten(z_decoded)\n",
    "        \n",
    "        # Reconstruction loss (as we used sigmoid activation we can use binarycrossentropy)\n",
    "        recon_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
    "        \n",
    "        # KL divergence\n",
    "        kl_loss = -5e-4 * K.mean(1 + z_sigma - K.square(z_mu) - K.exp(z_sigma), axis=-1)\n",
    "        return K.mean(recon_loss + kl_loss)\n",
    "\n",
    "    # add custom loss to the class\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        z_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, z_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "\n",
    "\n",
    "y = CustomLayer()([input_img, z_decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(input_img, y, name = 'vae')\n",
    "vae.load_weights(os.getcwd() + \"\\\\models\\\\vae.h5\")\n",
    "encoder = Model(vae.input, vae.layers[15].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesses data as before, but puts markers inbetween to seperate the data\n",
    "\n",
    "# TODO: might be times between photos within the folders, so maybe will need to look closer at where to put delimiters\n",
    "\n",
    "os.chdir(\"images\")\n",
    "\n",
    "# save boolean of if data has been saved already or not so can negate future\n",
    "#    cells to avoid the code breaking\n",
    "data_exists = os.path.exists(\"train_data_rnn.z\")\n",
    "# constant for sizing\n",
    "IMG_SIZE = 128\n",
    "\n",
    "if data_exists:\n",
    "    print(\"train_data_rnn.z already exists, if this notebook is run to completion the old data will be replaced.\")\n",
    "    \n",
    "data = []\n",
    "path = os.getcwd()\n",
    "print(path)\n",
    "\n",
    "def create_data():   \n",
    "    count = 0\n",
    "    for folder in os.listdir(path):\n",
    "        if \"train_data\" in folder:  # skips any train data files, as that should be the only non-directory item in images\n",
    "            continue\n",
    "        print(\"FOLDER: \",folder)\n",
    "        # added + \"/\" + to below to make it work\n",
    "        for filename in os.listdir(path + \"\\\\\" + folder):\n",
    "            # changed to NEF (That's what I have the images saved as, may need to change back to JPG in future)\n",
    "            if(\".NEF\" in filename):\n",
    "                # added slash here too\n",
    "                temp_path = path + \"/\" + folder + \"/\" + filename\n",
    "                count += 1\n",
    "                try:\n",
    "                    img_array = cv2.imread(temp_path)\n",
    "                    img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2GRAY)\n",
    "                    img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
    "                    data.append(img_array)\n",
    "                    print(\"image processed...\" + str(count) , end=\"\\r\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        data.append(\"|\")\n",
    "\n",
    "create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data\n",
    "print(train_data.shape[2])\n",
    "\n",
    "# Reshape \n",
    "img_width  = train_data.shape[1]\n",
    "img_height = train_data.shape[2]\n",
    "num_channels = 1\n",
    "x_train = train_data.reshape(train_data.shape[0], img_height, img_width, num_channels)\n",
    "\n",
    "\n",
    "input_shape = (img_height, img_width, num_channels)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goes through the seperated data and pairs it\n",
    "paired_data = []\n",
    "\n",
    "for i in range(len(x_train)-1):\n",
    "    if x_train[i] == \"|\" or x_train[i+1] == \"|\":\n",
    "        continue\n",
    "    else:\n",
    "        paired_data.append([x_train[i], x_train[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paired_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs the paired data through the encoder to get the latent vectors\n",
    "z_vals = []\n",
    "\n",
    "for pair in paired_data:\n",
    "    input1 = pair[0][None,:,:,:]\n",
    "    input2 = pair[1][None,:,:,:]\n",
    "    z1 = encoder.predict(input1)\n",
    "    z2 = encoder.predict(input2)\n",
    "    z_vals.append([z1, z2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finished, now save the data\n",
    "joblib.dump(z_vals, \"train_data_rnn.z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Trainer\n",
    "(As opposed to the controller part of the model)\n",
    "\n",
    "## Loading stuff into the kernel:\n",
    "Imports and training data (don't need the encoder, train data is already converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1.keras.backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape, MaxPooling2D,LSTM, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import save_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import joblib\n",
    "import cv2\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = joblib.load(\"images/train_data_rnn.z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array([np.array(p[0])for p in data])\n",
    "answers = np.array([np.array(p[1]) for p in data])\n",
    "print(train_data[0])\n",
    "print(answers[0])\n",
    "print(np.shape(train_data[0]))\n",
    "z_len = np.shape(train_data[0])[-1]\n",
    "print(z_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_rnn = Input(shape=(1,z_len))\n",
    "\n",
    "x = LSTM(z_len, return_sequences=True)(input_to_rnn)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(z_len))(x)\n",
    "x = Dropout(0.2)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Model(input_to_rnn, output, name='rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(answers[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "after testing with z length = 2048, 10 epochs seems like the best. val_loss goes from 0.90 to 0.82, but goes back to back to 0.83 for overfitting. If more data added, could potentially do more epochs also, doesn't take that long to run due to small dimensionality of inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.fit(train_data, answers, epochs=10, verbose=1, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.save_weights('models/rnn.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
